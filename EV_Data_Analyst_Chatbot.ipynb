{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# --- SECTION 1: Setup and Imports ---\n",
        "# This code is designed to be run in Google Colab.\n",
        "\n",
        "# Install necessary libraries (run this block first in Colab)\n",
        "# !pip install -q gradio pandas requests\n",
        "\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "\n",
        "# --- SECTION 2: Configuration and Data Loading ---\n",
        "\n",
        "# CRITICAL: Replace \"YOUR_GEMINI_API_KEY\" with your actual key\n",
        "# You can get a key from Google AI Studio.\n",
        "API_KEY = \"YOUR_GEMINI_API_KEY\"\n",
        "\n",
        "# The file uploaded by the user\n",
        "DATA_FILE = \"/content/ElectricCarData_Clean.csv\"\n",
        "\n",
        "# Gemini API Endpoint\n",
        "# Using gemini-2.5-flash-preview-09-2025 as the model for grounded generation\n",
        "API_URL = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-09-2025:generateContent?key={API_KEY}\"\n",
        "\n",
        "# Load and prepare the electric car data for context\n",
        "try:\n",
        "    df_cars = pd.read_csv(DATA_FILE)\n",
        "    # Convert the DataFrame to a Markdown table string for easy LLM interpretation\n",
        "    DATA_CONTEXT = df_cars.to_markdown(index=False, numalign=\"left\", stralign=\"left\")\n",
        "\n",
        "    # Define a focused system instruction\n",
        "    SYSTEM_INSTRUCTION = (\n",
        "        \"You are an expert Electric Vehicle (EV) Data Analyst. Your knowledge base is strictly limited \"\n",
        "        \"to the provided CSV data context below. Your goal is to answer user questions about EV \"\n",
        "        \"performance, range, and pricing ONLY based on this data. \"\n",
        "        \"If the data does not contain the answer, state that clearly. \"\n",
        "        \"The data is provided in Markdown table format.\"\n",
        "        f\"\\n\\n--- DATA CONTEXT ---\\n{DATA_CONTEXT}\\n--- END DATA CONTEXT ---\\n\"\n",
        "    )\n",
        "    print(f\"Successfully loaded {DATA_FILE} with {len(df_cars)} records.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    DATA_CONTEXT = \"Error: Data file not found. I cannot perform data analysis.\"\n",
        "    SYSTEM_INSTRUCTION = \"You are a helpful chatbot. I am currently experiencing a data loading error.\"\n",
        "    print(f\"Error: {DATA_FILE} not found. Running in generic chatbot mode.\")\n",
        "except Exception as e:\n",
        "    DATA_CONTEXT = f\"Error: Failed to process data file. {e}\"\n",
        "    SYSTEM_INSTRUCTION = \"You are a helpful chatbot. I am currently experiencing a data processing error.\"\n",
        "    print(f\"Error processing data: {e}. Running in generic chatbot mode.\")\n",
        "\n",
        "\n",
        "# --- SECTION 3: Gemini API Call Function ---\n",
        "\n",
        "def call_gemini_api(prompt, chat_history, system_instruction):\n",
        "    \"\"\"\n",
        "    Makes a POST request to the Gemini API with exponential backoff.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert Gradio history into the format expected by the Gemini API\n",
        "    contents = []\n",
        "    for user_msg, model_msg in chat_history:\n",
        "        # User message\n",
        "        contents.append({\n",
        "            \"role\": \"user\",\n",
        "            \"parts\": [{\"text\": user_msg}]\n",
        "        })\n",
        "        # Model message\n",
        "        contents.append({\n",
        "            \"role\": \"model\",\n",
        "            \"parts\": [{\"text\": model_msg}]\n",
        "        })\n",
        "\n",
        "    # Append the current user prompt\n",
        "    contents.append({\n",
        "        \"role\": \"user\",\n",
        "        \"parts\": [{\"text\": prompt}]\n",
        "    })\n",
        "\n",
        "    # Construct the payload\n",
        "    payload = {\n",
        "        \"contents\": contents,\n",
        "        \"systemInstruction\": {\n",
        "            \"parts\": [{\"text\": system_instruction}]\n",
        "        },\n",
        "        # Enable Google Search grounding only if we couldn't load the EV data\n",
        "        \"tools\": [] if DATA_CONTEXT.startswith(\"Error\") else []\n",
        "    }\n",
        "\n",
        "    # Exponential backoff logic\n",
        "    max_retries = 5\n",
        "    retry_delay = 1.0\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            headers = {'Content-Type': 'application/json'}\n",
        "            response = requests.post(API_URL, headers=headers, data=json.dumps(payload))\n",
        "            response.raise_for_status() # Raise exception for bad status codes (4xx or 5xx)\n",
        "\n",
        "            result = response.json()\n",
        "\n",
        "            # Extract generated text\n",
        "            candidate = result.get('candidates', [{}])[0]\n",
        "            text = candidate.get('content', {}).get('parts', [{}])[0].get('text', 'API Response Error: Could not extract text.')\n",
        "            return text\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                print(f\"API request failed (Attempt {attempt+1}/{max_retries}): {e}. Retrying in {retry_delay:.2f}s...\")\n",
        "                time.sleep(retry_delay)\n",
        "                retry_delay *= 2 # Exponential increase\n",
        "            else:\n",
        "                return f\"API Error after multiple retries: Could not reach the service. Details: {e}\"\n",
        "        except json.JSONDecodeError:\n",
        "            return \"API Error: Failed to decode JSON response.\"\n",
        "        except Exception as e:\n",
        "            return f\"An unexpected error occurred: {e}\"\n",
        "\n",
        "\n",
        "# --- SECTION 4: Gradio Chat Function ---\n",
        "\n",
        "def respond(message, history):\n",
        "    \"\"\"\n",
        "    The main chat function for Gradio.\n",
        "    \"\"\"\n",
        "    if API_KEY == \"YOUR_GEMINI_API_KEY\" or not API_KEY:\n",
        "        return \"Please set your Gemini API Key in the `API_KEY` variable inside the Python code to enable the chatbot.\"\n",
        "\n",
        "    print(f\"User: {message}\")\n",
        "\n",
        "    # Call the Gemini model with the user's message, history, and the system instruction (which includes the data)\n",
        "    model_response = call_gemini_api(\n",
        "        prompt=message,\n",
        "        chat_history=history,\n",
        "        system_instruction=SYSTEM_INSTRUCTION\n",
        "    )\n",
        "\n",
        "    print(f\"Gemini: {model_response}\")\n",
        "    return model_response\n",
        "\n",
        "# --- SECTION 5: Gradio Interface Setup and Launch ---\n",
        "\n",
        "# Create the Gradio ChatInterface\n",
        "iface = gr.ChatInterface(\n",
        "    fn=respond,\n",
        "    title=\"Electric Car Chatbot\",\n",
        "    chatbot=gr.Chatbot(height=500),\n",
        "    textbox=gr.Textbox(placeholder=\"Ask about range, price, or performance data for the cars in the CSV...\", container=False, scale=7),\n",
        "    theme=\"soft\",\n",
        "    description=\"I am an expert analyst. I can answer questions about the uploaded EV data based *only* on the contents of the `ElectricCarData_Clean.csv` file.\",\n",
        ")\n",
        "\n",
        "# Launch the app with share=True to get a public URL for the website output\n",
        "if __name__ == \"__main__\":\n",
        "    iface.launch(\n",
        "        share=True,\n",
        "        debug=True,\n",
        "        # We need to set max_threads to 1 because Gradio's internal threading can cause issues\n",
        "        # when calling external APIs rapidly in certain environments like Colab.\n",
        "        max_threads=1\n",
        "    )"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded /content/ElectricCarData_Clean.csv with 103 records.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1388069095.py:148: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot=gr.Chatbot(height=500),\n",
            "/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:330: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://52641637544872cbf0.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://52641637544872cbf0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "yLpe0mrKc2Tl",
        "outputId": "db7e417f-293a-4866-9646-e144fa3f9e34"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}